{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import itertools\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./auxiliary_func_noprint.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose input/output dimensions and look at the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oscillon': '[dduddt-0.5**2*u+2.*u**3.]',\n",
       " 'mat': '[dduddt+(1.-0.4*tf.cos(2.*t))*u]',\n",
       " 'exp': '[dudt+0.5*u]',\n",
       " 'wave': '[dduddt+25.*u]',\n",
       " 'dho': '[dduddt+0.5*dudt+25.*u]',\n",
       " 'linear': '[dudt-1.]',\n",
       " 'delay': '[dudt-0.5*u+1.*du]',\n",
       " 'gaussian': '[dudt+0.2*t*u]',\n",
       " 'stiff': '[dudt+21.*u-tf.exp(-t)]',\n",
       " 'twofreq': '[dduddt+u+2.*tf.cos(5.*t)+6.*tf.sin(10.*t)]'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Allowed input/output values in this preliminary version are (ni=2 no=1) (ni=3 no=1) (ni=3 no=2) and (ni=1 no=1) \n",
    "\n",
    "ni=1 #number of input variables  \n",
    "no=1 #number of output solutions \n",
    "\n",
    "#LOOK AT THE PDE DICTIONARIES MATCHING THE DIMENSION REQUIREMENTS\n",
    "#################################################################################\n",
    "### outputfunction is named u, v\n",
    "### first derivatives are named dudt, dudx, dudy\n",
    "### pure second derivatives are named dduddt, dduddx, dduddy\n",
    "### mixed second derivative (as ddudtdx) need to be coded in the loss function, we will provide an automatic derivation in future updates.\n",
    "############## NOTATION #####################\n",
    "## eomdict: impose PDE constraints. The form needs to be '[ eq1 , eq2 ]' \n",
    "## ICdict: Boundary conditions on t belonging to [t0,tL].\n",
    "##         The form needs to be [ '[ u - u(t0,x,y),dudt - dudt(t0,x,y)]' , '[u - u(tL,x,y),dudt - dudt(tL,x,y)]' ] \n",
    "##         If no condition needs to be provided on some boundary just replace (u - u(t,x,y0) ) with  [none].\n",
    "## boardx: Boundary conditions on x belonging to [x0,xL].\n",
    "##         The form needs to be [ '[u - u(t,x0,y),dudx - dudx(t,x0,y)]' , '[u - u(t,xL,y),dudx - dudx(t,xL,y)]' ]   \n",
    "##         If no condition needs to be provided on some boundary just replace (u - u(t,x,y0) ) with  [none].\n",
    "## boardy: Boundary conditions in y belonging to [y0,yL].\n",
    "##         The form needs to be [ '[u - u(t,x,y0),dudy - dudy(t,x,y0)]' , '[u - u(t,x,yL),dudy - dudy(t,x,yL)]' ] \n",
    "##         If no condition needs to be provided on some boundary just write  [none].\n",
    "## wdict:  weights to be used in loss function\n",
    "##         The form needs to be [w_bulk,w_IC,w_board]\n",
    "\n",
    "eqname_dict=eqname_gen(ni,no)\n",
    "analytic=eqname_analytic(ni,no)\n",
    "sol=sol_analytic(ni,no)\n",
    "\n",
    "if ni==1:\n",
    "    eomdict,ICdict=dictgen(ni,no)\n",
    "elif ni==2:\n",
    "    eomdict,ICdict,boardx,wdict=dictgen(ni,no)\n",
    "elif ni==3:\n",
    "    eomdict,ICdict,boardx,boardy,wdict=dictgen(ni,no)\n",
    "\n",
    "eomdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#### Choose the equation of motion to be studied\n",
    "#####################################################\n",
    "### 3to1 D equations studied: ['waveb','wave','wavet', 'poisson1', 'poisson2', 'poisson3','heat1','heat2']\n",
    "### 3to2 D equations studied: ['GT','LO','NS']\n",
    "### 2to1 D equations studied: [{'wave','waveN','twave','heat','heat2f','heat0','poisson','poisson1','AdvectionDiffusion','Burgers','poisson_par','par']\n",
    "### 1to1 D equations studied: [{'oscillon', 'mat', 'exp', 'wave', 'dho', 'linear', 'delay', 'gaussian', 'stiff', 'twofreq'}]\n",
    "#####\n",
    "##Create the list of coordinate boundaries, i.e. boundaries=[t_0,t_max,x_0,x_max,y_0,y_max]\n",
    "#### Boudaries used in the paper\n",
    "#bd=[0.,1.,0.,1.,0.,1.]               # 3to1 D\n",
    "#bd=[0.,1.,0.,2.*np.pi,0.,2.*np.pi]   #'GT'\n",
    "#bd=[0.1,1.,-2.,2.,-2.,2.]            #'LO'\n",
    "#bd=[0.,10.,0.,1.,0.,1.]              #'NS'\n",
    "#bd=[0.,1.,0.,1.]                     # 2to1 D\n",
    "#bd=[0.,20.]                          # 1to1 D\n",
    "\n",
    "bd=[0.,20.]\n",
    "\n",
    "\n",
    "\n",
    "## define number of points to be sampled in the bulk, the IC and on each boundary surface\n",
    "## for the 1D case (n_bulk, n_IC) will evaluate to (batch_sz-2,2) for the training part of adam\n",
    "n_bulk=2000\n",
    "n_IC=1\n",
    "batch_sz=256\n",
    "\n",
    "\n",
    "##Initialise the network\n",
    "n_l=35 #number of neurons per branch\n",
    "\n",
    "#create base frequencies to initialize the network\n",
    "pi=tf.constant(np.pi,dtype=tf.float32)\n",
    "none=tf.constant(0.,dtype=tf.float32)\n",
    "freq_t=base_freq(ni,bd)\n",
    "step=1.\n",
    "\n",
    "model=dNNsolve(ni,no,n_l,bd,step)\n",
    "\n",
    "model.save_weights(f\"model_seq_{ni}D_{n_l}nodes_randomMiniBatch.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t   log10 loss:   -0.37 \n",
      "Epoch 100:\t   log10 loss:   -0.65 \n",
      "Epoch 200:\t   log10 loss:   -1.20 \n",
      "Epoch 300:\t   log10 loss:   -1.35 \n",
      "Epoch 400:\t   log10 loss:   -1.68 \n",
      "Epoch 500:\t   log10 loss:   -1.77 \n",
      "Epoch 600:\t   log10 loss:   -1.78 \n",
      "Epoch 700:\t   log10 loss:   -1.79 \n",
      "Epoch 800:\t   log10 loss:   -1.78 \n",
      "Epoch 900:\t   log10 loss:   -1.77 \n",
      "Epoch 1000:\t   log10 loss:   -1.79 \n",
      "Epoch 0:\t   log10 loss:   -0.45 \n",
      "Epoch 100:\t   log10 loss:   -0.82 \n",
      "Epoch 200:\t   log10 loss:   -1.09 \n",
      "Epoch 300:\t   log10 loss:   -1.36 \n",
      "Epoch 400:\t   log10 loss:   -1.41 \n",
      "Epoch 500:\t   log10 loss:   -1.43 \n",
      "Epoch 600:\t   log10 loss:   -1.46 \n",
      "Epoch 700:\t   log10 loss:   -1.42 \n",
      "Epoch 800:\t   log10 loss:   -1.43 \n",
      "Epoch 900:\t   log10 loss:   -1.44 \n",
      "Epoch 1000:\t   log10 loss:   -1.42 \n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function random_sampling.<locals>.random_sampling_1D at 0x162ec5a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function random_sampling.<locals>.random_sampling_1D at 0x162ec4c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 0:\t   log10 loss:   -0.45 \n",
      "Epoch 100:\t   log10 loss:   -1.07 \n",
      "Epoch 200:\t   log10 loss:   -1.44 \n",
      "Epoch 300:\t   log10 loss:   -1.70 \n",
      "Epoch 400:\t   log10 loss:   -2.29 \n",
      "Epoch 500:\t   log10 loss:   -2.73 \n",
      "Epoch 600:\t   log10 loss:   -3.14 \n",
      "Epoch 700:\t   log10 loss:   -3.45 \n",
      "Epoch 800:\t   log10 loss:   -3.51 \n",
      "Epoch 900:\t   log10 loss:   -3.69 \n",
      "Epoch 1000:\t   log10 loss:   -3.50 \n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function random_sampling.<locals>.random_sampling_1D at 0x1624d4830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function random_sampling.<locals>.random_sampling_1D at 0x161fa5ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 0:\t   log10 loss:   -0.42 \n",
      "Epoch 100:\t   log10 loss:   -0.61 \n",
      "Epoch 200:\t   log10 loss:   -1.07 \n",
      "Epoch 300:\t   log10 loss:   -1.26 \n",
      "Epoch 400:\t   log10 loss:   -1.62 \n",
      "Epoch 500:\t   log10 loss:   -1.95 \n",
      "Epoch 600:\t   log10 loss:   -2.25 \n",
      "Epoch 700:\t   log10 loss:   -2.75 \n",
      "Epoch 800:\t   log10 loss:   -2.80 \n",
      "Epoch 900:\t   log10 loss:   -2.97 \n",
      "Epoch 1000:\t   log10 loss:   -2.90 \n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function random_sampling.<locals>.random_sampling_1D at 0x1630ae200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function random_sampling.<locals>.random_sampling_1D at 0x1630ae200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 0:\t   log10 loss:   -0.42 \n",
      "Epoch 100:\t   log10 loss:   -0.74 \n",
      "Epoch 200:\t   log10 loss:   -1.36 \n",
      "Epoch 300:\t   log10 loss:   -1.59 \n",
      "Epoch 400:\t   log10 loss:   -1.72 \n",
      "Epoch 500:\t   log10 loss:   -1.88 \n",
      "Epoch 600:\t   log10 loss:   -2.01 \n",
      "Epoch 700:\t   log10 loss:   -2.05 \n",
      "Epoch 800:\t   log10 loss:   -2.06 \n",
      "Epoch 900:\t   log10 loss:   -2.07 \n",
      "Epoch 1000:\t   log10 loss:   -2.06 \n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function function_factory.<locals>.assign_new_model_parameters at 0x162a36b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x16380b710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x16380b710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 0:\t   log10 loss:   -0.07 \n",
      "Epoch 100:\t   log10 loss:   -0.66 \n",
      "Epoch 200:\t   log10 loss:   -1.25 \n",
      "Epoch 300:\t   log10 loss:   -1.95 \n",
      "Epoch 400:\t   log10 loss:   -2.32 \n",
      "Epoch 500:\t   log10 loss:   -2.73 \n",
      "Epoch 600:\t   log10 loss:   -3.03 \n",
      "Epoch 700:\t   log10 loss:   -3.04 \n",
      "Epoch 800:\t   log10 loss:   -3.26 \n",
      "Epoch 900:\t   log10 loss:   -3.16 \n",
      "Epoch 1000:\t   log10 loss:   -3.26 \n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function function_factory.<locals>.assign_new_model_parameters at 0x16336aef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x162fa5830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x162e1cc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 0:\t   log10 loss:   -0.32 \n",
      "Epoch 100:\t   log10 loss:   -0.86 \n",
      "Epoch 200:\t   log10 loss:   -1.72 \n",
      "Epoch 300:\t   log10 loss:   -1.93 \n",
      "Epoch 400:\t   log10 loss:   -2.21 \n",
      "Epoch 500:\t   log10 loss:   -2.83 \n",
      "Epoch 600:\t   log10 loss:   -2.98 \n",
      "Epoch 700:\t   log10 loss:   -3.04 \n",
      "Epoch 800:\t   log10 loss:   -3.12 \n",
      "Epoch 900:\t   log10 loss:   -3.01 \n",
      "Epoch 1000:\t   log10 loss:   -3.08 \n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function function_factory.<locals>.assign_new_model_parameters at 0x162fa5f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x162312200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x162312290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Garfield/anaconda3/envs/tensor_flow/lib/python3.7/site-packages/ddeint/ddeint.py:145: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array([g(tt[0])] + results)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t   log10 loss:   -0.45 \n",
      "Epoch 100:\t   log10 loss:   -0.91 \n",
      "Epoch 200:\t   log10 loss:   -1.29 \n",
      "Epoch 300:\t   log10 loss:   -2.13 \n",
      "Epoch 400:\t   log10 loss:   -2.58 \n",
      "Epoch 500:\t   log10 loss:   -3.02 \n",
      "Epoch 600:\t   log10 loss:   -3.29 \n",
      "Epoch 700:\t   log10 loss:   -3.62 \n",
      "Epoch 800:\t   log10 loss:   -3.64 \n",
      "Epoch 900:\t   log10 loss:   -3.79 \n",
      "Epoch 1000:\t   log10 loss:   -3.90 \n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function function_factory.<locals>.assign_new_model_parameters at 0x162312830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x1628cc710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x1628cc710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 0:\t   log10 loss:   -0.37 \n",
      "Epoch 100:\t   log10 loss:   -0.22 \n",
      "Epoch 200:\t   log10 loss:   -0.41 \n",
      "Epoch 300:\t   log10 loss:   -0.59 \n",
      "Epoch 400:\t   log10 loss:   -0.49 \n",
      "Epoch 500:\t   log10 loss:   -0.87 \n",
      "Epoch 600:\t   log10 loss:   -1.02 \n",
      "Epoch 700:\t   log10 loss:   -0.96 \n",
      "Epoch 800:\t   log10 loss:   -0.91 \n",
      "Epoch 900:\t   log10 loss:   -0.55 \n",
      "Epoch 1000:\t   log10 loss:   -0.64 \n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function function_factory.<locals>.assign_new_model_parameters at 0x1638feb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x163288c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function random_sampling.<locals>.random_sampling_1D at 0x163288c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 0:\t   log10 loss:    0.43 \n",
      "Epoch 100:\t   log10 loss:    0.35 \n",
      "Epoch 200:\t   log10 loss:    0.34 \n",
      "Epoch 300:\t   log10 loss:    0.33 \n",
      "Epoch 400:\t   log10 loss:    0.33 \n",
      "Epoch 500:\t   log10 loss:    0.32 \n",
      "Epoch 600:\t   log10 loss:    0.31 \n",
      "Epoch 700:\t   log10 loss:    0.32 \n",
      "Epoch 800:\t   log10 loss:    0.34 \n",
      "Epoch 900:\t   log10 loss:    0.32 \n",
      "Epoch 1000:\t   log10 loss:    0.32 \n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function function_factory.<locals>.assign_new_model_parameters at 0x163d3bef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "epochs_adam=1001  \n",
    "\n",
    "f= open(\"1to1_results_randomMiniBatch.txt\",\"w+\")\n",
    "oneD_dict=['oscillon','mat','exp','wave','dho','linear','delay','gaussian','stiff','twofreq']\n",
    "\n",
    "\n",
    "for eom in oneD_dict:\n",
    "    \n",
    "    #Mini-batch sized prototype dataset for adam\n",
    "    X_data = (random_sampling(ni))(bd,batch_sz-2,1)\n",
    "    #Define ic, bc and bulk counters (I_t0,I_tL,I_bulk)\n",
    "    I=counters(ni)(bd,X_data)\n",
    "    [I_t0,I_tL,I_bulk]= tf.split(I,3,axis=1)\n",
    "    fake_output=tf.concat([X_data,I],axis=1)\n",
    "    step=2.\n",
    "    inputs=[X_data]\n",
    "    \n",
    "    #Larger dataset for BFGS\n",
    "    X_data_bfgs = (random_sampling(ni))(bd,n_bulk,n_IC)\n",
    "    I=counters(ni)(bd,X_data_bfgs)\n",
    "    fake_output_bfgs=tf.concat([X_data_bfgs,I],axis=1)\n",
    "\n",
    "    ########################\n",
    "    start=time.time() \n",
    "    #initialization of models\n",
    "    model.load_weights(f\"model_seq_{ni}D_{n_l}nodes_randomMiniBatch.h5\") #Restore weights to original values at each iteration\n",
    "    loss_fun=to_loss_1to1_minibatch(eom) \n",
    "    loss_fun_bfgs=to_loss_1to1(eom)  \n",
    "\n",
    "\n",
    "    # an educated guess for the oscillon central amplitude\n",
    "    if ni==1 and eom=='oscillon':\n",
    "        u0guess=tf.constant(0.6)\n",
    "\n",
    "    #Initialise BFGS method and compile the model\n",
    "    func = function_factory(model, loss_fun_bfgs, fake_output_bfgs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "                           loss=loss_fun,run_eagerly=False)\n",
    "\n",
    "    #Training with Adam\n",
    "    hist = model.fit(x=inputs, y=fake_output, batch_size = batch_sz, epochs=epochs_adam, verbose=0, callbacks = \n",
    "                       [Print_Loss_Every_so_many_Epochs_1D(),\n",
    "                       tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', mode='min', factor=1./2., patience=30, min_lr=1e-4)])\n",
    "\n",
    "\n",
    "    #Training with BFGS\n",
    "    init_params = tf.dynamic_stitch(func.idx, model.trainable_variables)\n",
    "    results = tfp.optimizer.bfgs_minimize(value_and_gradients_function=func, initial_position=init_params,tolerance=1e-20, max_iterations=5000)    \n",
    "    func.assign_new_model_parameters(results.position)\n",
    "\n",
    "\n",
    "\n",
    "    #Store loss data\n",
    "    loss_adam=np.array(hist.history['loss'],dtype=np.float32) \n",
    "    history_lbfgs=np.array(func.history)\n",
    "    hist_lbfgs=np.array(history_lbfgs[:,0])\n",
    "    loss=np.concatenate((loss_adam,hist_lbfgs))\n",
    "    end=time.time()\n",
    "\n",
    "    n_plt=300\n",
    "    pred, smse, tplt=points_plt_mse(ni,no,bd,model,sol) \n",
    "\n",
    "\n",
    "    eps=1e-20 ##small parameter to avoid inf values\n",
    "    f.write('\\n EOM: ' + eom)\n",
    "    f.write('\\n Neurons per branch:  %s' % (n_l))\n",
    "    f.write('\\n Epochs:  %s' % (loss.size))\n",
    "    f.write('\\n Total Time: %.4f' % (end-start))\n",
    "    f.write('\\n Final log10(Loss Adam): %.4f' % (np.log10(loss_adam[-1])+eps))\n",
    "    f.write('\\n Final log10(Loss): %.4f' % (np.log10(loss[-1])+eps))\n",
    "    f.write('\\n Log10 of  Sqrt Mean squared error:  %.4f' % (np.log10(smse+1e-20)+eps))\n",
    "    f.write('\\n log10(Loss bulk): %.4f log10(Loss IC): %.4f log10(Loss board): %.4f' % (np.log10(eps+np.array(history_lbfgs[:,1])[-1]),np.log10(eps+np.array(history_lbfgs[:,2])[-1]),np.log10(eps+np.array(history_lbfgs[:,3])[-1])))\n",
    "    f.write('\\n #######################################################')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
